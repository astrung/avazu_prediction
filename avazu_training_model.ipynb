{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import linear_model\n",
    "import time\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_X_and_Y(training, test):\n",
    "    Y_train = training[['click']]\n",
    "    del training['click']\n",
    "    X_train = training\n",
    "\n",
    "    Y_test = test[['click']]\n",
    "    del test['click']\n",
    "    X_test = test\n",
    "    return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_outliner(data):\n",
    "    for col in ['C18_scaler', 'C20_scaler', 'C21_scaler']:\n",
    "        # keep only the ones that are within +3 to -3 standard deviations in the column col,\n",
    "        # print(data.head(10)[col])\n",
    "        data = data[np.abs(data[col] - data[col].mean()) <= (3 * data[col].std())]\n",
    "        # or if you prefer the other way around\n",
    "        # data = data[(np.abs(data[col] - data[col].mean()) > (3 * data[col].std()))]\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_time_stamp_feature(data_frame):\n",
    "    data_frame['day'] = data_frame['hour'].apply(lambda x: (x - x % 10000) / 1000000)  # day\n",
    "    data_frame['dow'] = data_frame['hour'].apply(lambda x: ((x - x % 10000) / 1000000) % 7)  # day of week\n",
    "    data_frame['hour'] = data_frame['hour'].apply(lambda x: x % 10000 / 100)  # hour\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hashing_data(data_frame, column_name, hasher, number_of_features):\n",
    "    list_column_name = []\n",
    "    for i in range(number_of_features):\n",
    "        list_column_name.append(column_name + str(i))\n",
    "    new_set_of_columns = hasher.transform(data_frame[column_name].values)\n",
    "    test = pd.DataFrame(new_set_of_columns.toarray(), columns=list_column_name)\n",
    "    result = pd.concat([data_frame, test], axis=1)\n",
    "    del result[column_name]\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_predict(test_data, hasher, list_hash_columns,number_hash_feature):\n",
    "    Y_id = test_data[['id']]\n",
    "    del test_data['id']\n",
    "    test_data = test_data\n",
    "    for column_name in list_hash_columns:\n",
    "        test_data = hashing_data(test_data, column_name, hasher,number_hash_feature)\n",
    "    test_data = extract_time_stamp_feature(test_data)\n",
    "\n",
    "    # result_data = sgd.predict(test_data)\n",
    "    raw_result = sgd.predict_proba(test_data)\n",
    "\n",
    "    predict_frame = pd.DataFrame(raw_result, columns=['non_click', 'click'])\n",
    "    # predict_frame = pd.DataFrame(result_data, columns=['click'])\n",
    "    del predict_frame['non_click']\n",
    "    result = pd.concat([Y_id, predict_frame], axis=1)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_predict_csv(write_index, data, path, hasher, list_hash_columns,number_hash_feature):\n",
    "    data = data.reset_index(drop=True)\n",
    "    new_scaler_data = get_predict(data, hasher, list_hash_columns,number_hash_feature)\n",
    "    \n",
    "    if write_index == 0:\n",
    "        new_scaler_data.to_csv(path, mode='a', index=False\n",
    "                               # ,float_format='string'\n",
    "                               )\n",
    "    else:\n",
    "        new_scaler_data.to_csv(path, mode='a', header=False,\n",
    "                               index=False\n",
    "                               # ,float_format='string'\n",
    "                               )\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_classes = np.array([0, 1])\n",
    "batch_size = 100000\n",
    "list_hash_columns = ['site_id', 'site_domain', 'site_category', 'app_id', 'app_domain', 'app_category',\n",
    "                     'device_model',\n",
    "                     'device_id', 'device_ip']\n",
    "\n",
    "chunks = pd.read_table('C:\\\\Users\\\\tuana\\\\Desktop\\\\kaggle\\\\scale_train.csv', chunksize=batch_size, sep=','\n",
    "                       , usecols=range(1, 24), index_col=False)\n",
    "split_data = True\n",
    "global_test_set = None\n",
    "sgd = SGDClassifier(n_jobs=-1, tol=0.00001, loss=\"log\",alpha=0.001)\n",
    "number_of_hashing_feature = 10\n",
    "hasher = FeatureHasher(n_features=number_of_hashing_feature, input_type='string')\n",
    "index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tuana\\Anaconda2\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2010: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sgd : 0.825660 \n",
      "sgd : 0.518940 \n",
      "sgd : 0.824700 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-1d07e9943c69>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcolumn_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlist_hash_columns\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mtraining\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhashing_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumn_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhasher\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_hashing_feature\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m         \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhashing_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumn_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhasher\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_hashing_feature\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0mtraining\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_time_stamp_feature\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-2e260a313dd7>\u001b[0m in \u001b[0;36mhashing_data\u001b[1;34m(data_frame, column_name, hasher, number_of_features)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mnew_set_of_columns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhasher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_frame\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumn_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_set_of_columns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlist_column_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdata_frame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[1;32mdel\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcolumn_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\tuana\\Anaconda2\\lib\\site-packages\\pandas\\core\\reshape\\concat.pyc\u001b[0m in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, copy)\u001b[0m\n\u001b[0;32m    205\u001b[0m                        \u001b[0mverify_integrity\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m                        copy=copy)\n\u001b[1;32m--> 207\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    208\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\tuana\\Anaconda2\\lib\\site-packages\\pandas\\core\\reshape\\concat.pyc\u001b[0m in \u001b[0;36mget_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    405\u001b[0m             new_data = concatenate_block_managers(\n\u001b[0;32m    406\u001b[0m                 \u001b[0mmgrs_indexers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew_axes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 407\u001b[1;33m                 copy=self.copy)\n\u001b[0m\u001b[0;32m    408\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    409\u001b[0m                 \u001b[0mnew_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\tuana\\Anaconda2\\lib\\site-packages\\pandas\\core\\internals.pyc\u001b[0m in \u001b[0;36mconcatenate_block_managers\u001b[1;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[0;32m   4830\u001b[0m     blocks = [make_block(\n\u001b[0;32m   4831\u001b[0m         \u001b[0mconcatenate_join_units\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjoin_units\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4832\u001b[1;33m         placement=placement) for placement, join_units in concat_plan]\n\u001b[0m\u001b[0;32m   4833\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4834\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mBlockManager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblocks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\tuana\\Anaconda2\\lib\\site-packages\\pandas\\core\\internals.pyc\u001b[0m in \u001b[0;36mconcatenate_join_units\u001b[1;34m(join_units, concat_axis, copy)\u001b[0m\n\u001b[0;32m   4943\u001b[0m         \u001b[0mconcat_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_concat\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4944\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mconcat_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4945\u001b[1;33m             \u001b[0mconcat_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconcat_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4946\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4947\u001b[0m         \u001b[0mconcat_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_concat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_concat_compat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_concat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconcat_axis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for data in chunks:\n",
    "    if split_data:\n",
    "        training, test = train_test_split(data, shuffle=True, train_size=0.5)\n",
    "        training = data\n",
    "        global_test_set = test.copy()\n",
    "        split_data = False\n",
    "    else:\n",
    "        training = data\n",
    "        test = global_test_set\n",
    "\n",
    "    training = training.reset_index(drop=True)\n",
    "    test = test.reset_index(drop=True)\n",
    "\n",
    "    for column_name in list_hash_columns:\n",
    "        training = hashing_data(training, column_name, hasher, number_of_hashing_feature)\n",
    "        test = hashing_data(test, column_name, hasher, number_of_hashing_feature)\n",
    "\n",
    "    training = extract_time_stamp_feature(training)\n",
    "    test = extract_time_stamp_feature(test)\n",
    "\n",
    "    X_train, Y_train, X_test, Y_test = split_X_and_Y(training, test)\n",
    "\n",
    "    sgd.partial_fit(X_train, Y_train.values.ravel(), classes=all_classes)\n",
    "    score = sgd.score(X_test, Y_test.values.ravel())\n",
    "    print(\"sgd : %f \" % score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "joblib.dump(sgd, 'model_submit.pkl')\n",
    "# sgd = joblib.load('model_submit.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_chunks = pd.read_table('C:\\\\Users\\\\tuana\\\\Desktop\\\\kaggle\\\\scale_test.csv', chunksize=batch_size, sep=',',\n",
    "                            index_col=False, dtype={'id': str})\n",
    "write_test_csv_index = 0\n",
    "for test_data in test_chunks:\n",
    "    write_predict_csv(write_test_csv_index, test_data, \"C:\\\\Users\\\\tuana\\\\Desktop\\\\kaggle\\\\result3.csv\", hasher,\n",
    "                      list_hash_columns,number_of_hashing_feature)\n",
    "\n",
    "    write_test_csv_index = write_test_csv_index + 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
